{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Multinomial Naive Bayes\n",
    "\n",
    "The purpose of this project is to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The goal is to write a program that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "To train the algorithm, we'll use a dataset of 5,572 SMS messages that are already classified by humans. The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the papers authored by Tiago A. Almeida and José María Gómez Hidalgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset\n",
    "\n",
    "The first step is to explore the data, starting with reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import operator\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SMS messages: 5,572\n",
      "Number of missing values in the dataframe: 0\n",
      "\n",
      "Spam vs. ham (non-spam), %\n",
      "+-------+------+\n",
      "| Label |  %   |\n",
      "+-------+------+\n",
      "|  ham  | 87.0 |\n",
      "| spam  | 13.0 |\n",
      "+-------+------+\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAACqCAYAAACjxUZOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXe0lEQVR4nO3dedxd073H8c83ZjWG1lSEmkrNpTX0mq5SQ1SLUkW4dVFa5Wpd5WrUVNNVbbVKe2lxTTVzTUG0lJpVjDWkildCJEFiiPC7f6x1mt3jnMeTPM8553nO+r5fr/3az1lr7b3XPuvJ89tr77VXFBGYmZlZOYZ0ugJmZmbWXg7+ZmZmhXHwNzMzK4yDv5mZWWEc/M3MzArj4G9mZlYYB38zG9AkjZQUkkZ3ui5m3cLB3wYtJTtLulLS3yS9LWmKpGcl3SnpvyXtKGmBTtfVrFUkLS7pN5LGSZom6WlJR0mavYdtFpU0QdJL/vdRpqa/HGYDmaSFgKuATSrJ04G3gGWA5YGNgEOAvYHz2lpBszaQtDBwF+n3HWAqsCJwLLAmsHOTTU8DFgG+EhFvtLqeNvC452+D1e9Igf990h+ylYC5ImIRYB7SH77DgUc6VkOz1juEFPgfAZaNiPmAzYEpwE6SNq3fQNLmwJ7A1RFxZfuqagOJg78NOpJWBLbPH4+KiMMi4q8R8QFAREyPiL9ExMkRsRZwSafqatZiW+b1kRHxAkBE3A6ck9O/WC0saW7gLNLFwUHtqqQNPA7+NhitVfn56o8qHBFv16dJGpsHkY2QNL+kEyU9lccNTJB0laTPNdunpGUkHSjp+vyMdWoeb/C4pJ9IWqaHbUfnY4+UNJukQyQ9lLd/JR97zUr5efMz3DH5OK9JukTSpz7q3Bsc+5p87Cs+otyncrmQtHFd3laSrpD0Yn7G/Iak5yTdLOkwSUNntl4zeQ5b5O/9VUnvSHpC0g9zYGtUfh5JwyWdI+nhvN27kl7O3/WXejjWiPwdjM2fvyDp2txOU3O7/VvdNttKuiUf5y1J90n6Wr9+CTMsmtfP1qX/Na8/Xpd+FOmxwJER8WKL6mSDQUR48TKoFtJzzMjLlrO4j7F5+0OAJ/PP7wKvV/b9PrBPk+1HV8oFMDmXr37e+CO2PR64pXLsKZXt3wQ+S3ou+2BOe5s0pqFWZjywzEye906V4w3todwPc7nnAFXSj64776m5rtW0Tfu5vUfm/Y4Gvgd8kJdJeV077m3AbA22H1FXv7dyvatppzY5dm3bscA3cxt/kNu3uv2Jufwxld+d+jL7t+Dfwj1539vWpZ+W00+opK0KTAPuA4Z0+t+xl84uHa+AFy8zuwDDKn/0/wKsNAv7GFsJ0hNJFxSz57xPVwL0e8A6Dbb/OWlMwaeBeXLa7MD6wA1525dqeXXb1vY9CZiQA/IcgID1SL24IA3kugJ4nnT7dkhetgBeyWUumMnzniufb4/BiNRzDOCYStqyzLjAOQ1YspK3ILAxcCawbj+398jK9/U+cAKwaM5boBJwgwYXa8CXgV8BmwKLVNKXIF3MTMvbDm+w7QhmXOS8C5wBfDznDSUNJK0F+++TBp0eCSxYOUbt92FKLb0fv5tj874fApbOaf8CvJHTN8tpAu7M9VurU/92vQycpeMV8OJlVhbg7Mof/A9IveMzgX2Az1DprTbZfmxl+y0a5M8DPJ3zr5/Jus1GGoAVwDca5I+uHPtDdwdIA7aqvdQVGpTZp5I/x0zW76y87Z+a5G9QOf4KlfRdctpTbW7rkZX6jGxS5vKcf8ss7P+wvO2oBnkjKsc+p0lbP1cpc2SDMgsw467Oh34f+vjdDK37Xa7ePbqiUm6/nHZKO9vOy8Bd/MzfBqtvkXo9U0m9mrVz2m+AR4Fx+T3/xT5iP3dFxK31iZHGCZySP24tacHeViwi3gduzB837qHonRFxZ4P0O0i9TIDfR8QzDcrclNfzkJ7hzozz83oDSSs0yN8jr++uO/bkvJ5f0sdm8pj94V3g1CZ5tbEfa8zCfq/P6w0kzdZDuR/XJ+S2rv3+vAP8pEGZN4C7+1C/piJiIrAh8FvS3aA5SXeORgJfgzQPQK77WNLjHCRtmefCeEvSZEmX54G0VggHfxuUIo3oPxpYihSsfk3qbU/LRT5Bep4/RtL6Pezqtl7kDQHWqc/Mg7/Ok/RkHqxXGyAXpFvAAJ/sYf/3NkrMAWVC/nhfk23HV35euIdjNNr/XcwYIPaNap6kOclBg/Q6ZdW9uV5LAH+WdJCkVSRpZo7fB49FxJQmeS/ndcPBhpIWk3SMpLvzgMnplbZ6PBebl+bf5cSIqB9UV1Nri8cjYupHlJmptuqNiHg5IkZExGIRMWdErBARx0TEe7nIGcBCwLci4i1Jw0mPItYmXTTdA3wF+FNPA1Wtuzj426AWEa9HxAURsW+k1/oWJL3+dG0usihwebOR4KTn8s1U8z5RzZB0EvAHYC9gZWBu0jPp8XmpBYGeeshv9pA3vacyETG98nGOHvbTTK33v0dd+jakAPouda9IRsRkYDfgVWA14GfAE8Ck/BbBNyTNSl16qzff14cmLpO0AWlQ59HA50nn9zappzyeGRda0Ly9Zrmt6sq08vv5kPwmwy7AxRFxQ76z8TPS44ptImK3iNiaNPh0UeDEdtbPOsfB37pKRLwTEaMiYjjpViik3vfWzTbpaXeNEiVtyYye/S+A1UkTDA2NiMUjYnHg9FrxmTqB9qkF/+UlbVRJr10MXBcRk+o3iohRwHKkSWJ+SxoYuCBp3oXzgYckLdWyWs8kpSluLyL1fB8mXdwsEBHz557y4qQLgn9s0vZKtoikeUm/n5OB7+bkdUkzYD4QEXdUip9K+n3fXpLjQgHcyNbNzq78vHKTMj3dlq/mvVL5ede8vikiDoyIMflWfdXivaxjR0TEc6S3CSAHfKWpYrfNaec32i5vOzUizs+3mlcifU+Hk5551+4IDBQbMOMthe0i4oaIqO+hD+i26oNjSG/GfD8iao8datMA/9MjjHxXZwIwPx+eG8C6kIO/dbPq8+F3m5TZrIfta3kfkF6lqlk6rx+igfwMfPPeVLDDas/0d5E0F+n28FykIPB/vd1JRLwUESeTXv+DGbPODQS1tno1Ipo94vnXdlWmXSStRert30kaD1Nvnh7SerobZl3Cwd8GHUnLSVqpF0X3qvz8YJMyG6vx/OdzA/+RP96Ue0Y1r+f1mk32uT8zelgD2aWki6KFge2Yccv/4spgsX/IFwg9qc2kWH8XpJNqbbVYozc/JH0S+E57q9Ra+bb92aSL1v0iohrMn8/rdVT5X/8krQbMRxq3UB0DYV3Kwd8Go9WAJ/IUr3tKGlbLkDSHpLUlnQscmpPvJfWAGnmdNCBwp9ofQ0mrkF7/WoUUyI6u26b2Gt+XJP1X7bU3SQtJ+gHptvdrfT7LFssXNLWBkUeQ/hdEaH7L/3BJN0jaIwdNIF0USNqFNPse1N01kDSs8ibEyH47gd65kxmvg15au2hUmlZ5K2bMudAxlSmEo9GF6Cw4iDRZ1EkR8Xhd3gOkgaxLAcdJmjM/7qk9qrku8v+RYd3Nwd8Go/dIv7vbkAadPZ/nan+N1JN9kDQ5C/nnHXv4g3YMafT6ZcAUSZNJI9hrE+0cEBH3123zO+CP+ecfAW9KmkgK+MeTLg5+2cdzbJfarf918/qpiGj4CiLpO986b/P3/I74a6Qe/yWkgX9PMOOiq+Mi4nXSJD6QZr57StKbpEdCN5LqvHeHqtfv8kXZcaQJqo6vz89viRxM+t0+nDQT4ATSI66JwA/aVlnrKAd/G3Qi4ibSxDYHk4L2E6SgvxBpxru/km5p7wqsFxEvN94TkF7PW580CcoLzJj+9lpgo4g4p36DfEv8i6QLh6dJFyMi3WE4ABjOwLr13ZMbSBc/NfXv9ledDfw7afT8GNJ3vQDpO/wj6RnzOhExriU1nUURcRZpIONoUtCfndT7/Rnp0c2jHatcUns7YgrwWB/39XPSoL39I6LhOJeIuJz0fdxNejQwBbgK2DAixvbx+DZI6J8fB5mVIf8vbcsCe0fEeZ2tjZVM0ijS/9dwXET8V6frY2Vwz9/MrEPyIMoNSXebmk1dbNbvHPzNzDrn86RX7E7O4xPM2uJDU2GamVl75Fn2umZWQRs83PM3MzMrjAf8mZmZFaak2/6+yjEzs9I0fKzk2/5mZmaFcfA3MzMrjIO/mZlZYRz8zczMCuPgb2ZmVhgHfzMzs8I4+JuZmRXGwd/MzKwwDv5mZmaFcfA3MzMrjIO/mZlZYRz8zczMCuPgb2ZmVphi/le/A3d/sNNV6KgzL1yn01UwM7MBwj1/MzOzwjj4m5mZFcbB38zMrDAO/mZmZoVx8DczMyuMg7+ZmVlhHPzNzMwK4+BvZmZWGAd/MzOzwvQq+EtaSdKVkl6R9I6kFyRdJml2SZtKCklflXSepEmS3pB0oaRF6vZzkKS7JU2UNFnSPZK2rSszLO9vf0knShon6U1JF0iaV9IKkm6SNEXSM5L26s8vxMzMrNv1dnrf64DJwAHABGApYBv++eLhJ8AoYDdgReAEYElgs0qZYcCvgbH52NsD10naJiJuqDvmEcBoYC9gVeBk4ANgbeAc4NRcn3Ml3R8Rj/XyXMzMzIr2kcFf0qKkYL5DRFxTyfrfnF/7/FhE7J1/vlHSROACSVtExK0AEXFYZb9DgFuBlYD9gfrg/2xE1Hr1N0n6ArAHsEdEXJD3cT8wHNgJcPA3MzPrhd7c9n8NeA74saR9Ja3YpNyldZ8vI/XUN6glSFpX0nWSxgPTgfeALYGVG+yv/mLgyby+qZYQEZOAV4Cle3EeZmZmRi+Cf0QEKUDfD5wIPC3pOUkH1BUdX7fdNGAS6REBkpYm9fSHAt8GNgTWA24E5m5w6El1n6f1kN5oezMzM2ugV8/8I+I5YE+le/xrAgcBv5A0Fng7F1usuo2kOYGFgZdy0tbAgsAuEfFipdy8fTkBMzMzmzkz9apfJA8Dh+akz1Syd6krvnPe/935cy3Iv1crIGklYKOZqYOZmZn1TW8G/K0BnAFcAjwDzAaMID2zvw2YPxddTdK5wMWkQXzHA3fUBvuR3gSYDvxO0mnAEsAxwAt4vgEzM7O26U3QHUcK0IcC1wAXkV7h2y4iHqiUOxgQ6SLhBNLrgTvVMvOreLsDy+b9fB/4T+APfT4LMzMz6zWl8Xx92IG0KXA7sGVEjOqHOrXEgbs/2LcTHeTOvHCdTlfBzMzaT40SfbvdzMysMA7+ZmZmhent9L5NRcRomtxWMDMzs4HHPX8zM7PCOPibmZkVxsHfzMysMA7+ZmZmhenze/6DSDEnamZmlvk9fzMzM3PwNzMzK46Dv5mZWWEc/M3MzArj4G9mZlYYB38zM7PCOPibmZkVxsHfzMysMA7+ZmZmhXHwNzMzK4yDv5mZWWEc/M3MzArj4G9mZlYYB38zM7PCOPibmZkVxsHfzMysMA7+ZmZmhXHwNzMzK4yDv5mZWWEc/M3MzArj4G9mZlYYB38zM7PCOPibmZkVxsHfzMysMA7+ZmZmhXHwNzMzK8zsna5Auxy4+4OdroKZmVlDZ164TluP556/mZlZYRz8zczMCuPgb2ZmVhgHfzMzs8I4+JuZmRXGwd/MzKwwDv5mZmaF6ZfgL2mkpJBUzLwBZmZmg5V7/mZmZoVx8DczMytMfwf/5SRdL2mKpL9JOlrSEABJc0s6XdKYnD9O0rWSVqnuQNKI/AhhQ0mXSnpT0nhJR+T8rSU9JGmqpPskrdvP52BmZtbV+jv4XwncBnwZuAo4Btgr580FzA8cB2wLHADMDdwjafEG+/ot8CiwY97XCZJOAk4BTgK+BnwMuErSnP18HmZmZl2rvwfonRYR5+afR0naHNgNODciXge+WSsoaTbgJmB8LnN63b7Oj4hjc9nRpIuAQ4GVIuL5nD4EuBrYALijn8/FzMysK/V3z//6us9jgGVqHyTtIunPkiYD04GpwHzAyg32dUPth4iYDjwDPF0L/NmTeb1036tuZmZWhv4O/hPrPr9LurWPpO2BS4AngK8DnwPWA16tlakzqe7ztCZpNNnezMzMGmjne/m7As9ExIhagqQ5gKFtrIOZmVnx2vmq37ykW/1VewCztbEOZmZmxWtnz/9G4MuSTgeuA9YFvgNMbmMdzMzMitfO4H8OaWDePsB+wH3A9qTXA83MzKxNFBGdrkNbHLj7g2WcqJmZDTpnXrhOq3atRome3tfMzKwwDv5mZmaFcfA3MzMrjIO/mZlZYRz8zczMCuPgb2ZmVphiXvUDijlRMzOzzK/6mZmZmYO/mZlZcRz8zczMCuPgb2ZmVhgHfzMzs8I4+JuZmRXGwd/MzKwwDv5mZmaFcfA3MzMrjIO/mZlZYRz8zczMCuPgb2ZmVhgHfzMzs8IU87/6SRoDvNPpehiLAhM6XQlzOwwAboOBodvbYUJEbF2fOHsnatIh70TEZztdidJJut/t0Hluh85zGwwMpbaDb/ubmZkVxsHfzMysMCUF/7M7XQED3A4Dhduh89wGA0OR7VDMgD8zMzNLSur5m5mZGV0e/CUtLen3kl6X9IakKyQt0+l6dStJO0m6XNLfJL0t6SlJJ0qav67cwpJ+LWmCpKmSRklavVP17naSbpQUko6rS3c7tJikbST9QdKU/DfofkmbV/LdBi0maSNJN0t6JbfBg5L2qStTXDt0bfCXNC9wG7AKsBewB7AicLukj3Wybl3sMOB94AfA1sAvgQOAWyQNAZAk4Jqc/23gq8AcpHb5ZCcq3c0k7Qas2SDd7dBikvYDrgYeAHYEdgYuA+bN+W6DFpO0BjCK9L3uS/qO7wN+I+mAXKbMdoiIrlyAg0mBaIVK2nLAdODQTtevGxfg4w3S9gQC2Dx/3iF/3qxSZkFgIvDTTp9DNy3AQsA4YLf8nR9XyXM7tPa7Hwa8DXy3hzJug9a3wwnANGC+uvR7gLtLboeu7fkDw4F7IuKZWkJEPA/cRWps62cR8WqD5Pvyeqm8Hg68HBG3V7Z7HbgWt0t/Oxl4LCIuapDndmitfYAPgLN6KOM2aL05gfdIF2JVk5lx57vIdujm4L8aMKZB+mPAqm2uS8k2yesn8rqndllG0nxtqVWXk7Qx6a7Lt5oUcTu01sbAk8Cukp6VNF3SM5IOrJRxG7TeeXn9U0lLSlpI0r7AFsDpOa/Idujm4D8UmNQgfSKwcJvrUiRJSwE/AkZFxP05uad2AbdNn0maA/gVcGpEPNWkmNuhtZYkjTE6Bfgx8EXgFuDnkg7OZdwGLRYRY4BNST34l0jf95nA/hFxcS5WZDt0+9z+jSYxUNtrUaB8tXw1aYzF3tUs3C6tdjgwD3B8D2XcDq01BJgfGBERV+S02yQNA46Q9FPcBi0naUXgclIvfn/S7f8dgLMkvRMRF1JoO3Rz8J9EuqKrtzCNr/Ksn0iamzR6dnlgk4h4sZI9kebtAm6bPsmvsh4JfBOYS9Jcley5JC0EvInbodVeI/X8b6lLv5k0qnwJ3AbtcALpmf92EfFeTrtV0iLAGZIuotB26Obb/o+RnuXUWxV4vM11KUa+5Xw5sD6wTUQ8Wlekp3Z5ISKmtLiK3W55YG7gAtIfrdoC6VXMScDquB1a7bEm6bXe5Ae4DdphdeCRSuCvuRdYBPgEhbZDNwf/a4DPS1q+lpBvuW2U86yf5Xf5LyQNptkhIu5pUOwaYClJm1S2WwDYHrdLf3gY2KzBAumCYDPgGdwOrXZlXm9Vl74V8GJEjMNt0A7jgLUkzVmX/jngHVKvv8h26Nq5/fNEPo+QnvEcRXqmcyzpOdwa3Xo110mSfkl6rnY8cF1d9osR8WK+QLgTWBr4HqknegSwBrBmRPy9jVUuhqQAjo+Io/Jnt0ML5YljbiVNsHQk8BywE2mimb0j4jy3QetJ2ok0sdLNwC9I8WA4cCBwekQcWmw7dHqigVYuwDKkW9BvkJ5zXgUM63S9unUBxpIushotIyvlhgL/Q7rqfov8R7LT9e/mhbpJftwObfnOFyCNLB9PmmjmL8DX3QZtb4cvAaOBV3MceJj0CuxsJbdD1/b8zczMrLFufuZvZmZmDTj4m5mZFcbB38zMrDAO/mZmZoVx8DczMyuMg7+ZmVlhHPzNzMwK4+BvZmZWGAd/MzOzwvw/2W7XyDF8CokAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "\n",
    "print(f'Number of SMS messages: {sms.shape[0]:,}')\n",
    "print(f'Number of missing values in the dataframe: {sms.isnull().sum().sum()}\\n')\n",
    "\n",
    "def pretty_print_table(df, substring):\n",
    "    '''Pretty-prints a table of the result of `value_counts` method (in % and\n",
    "    rounded) on the `Label` column of an input dataframe. Prints the title of\n",
    "    the table with an input substring incorporated.\n",
    "    '''\n",
    "    print(f'Spam vs. ham {substring}, %')\n",
    "    spam_ham_pct = round(df['Label'].value_counts(normalize=True)*100, 0)\n",
    "    print(spam_ham_pct.to_markdown(tablefmt='pretty', headers=['Label', '%']))\n",
    "\n",
    "# Pretty-printing % of spam and ham messages\n",
    "pretty_print_table(df=sms, substring='(non-spam)')\n",
    "\n",
    "# Plotting % of spam and ham messages\n",
    "spam_pct = round(sms['Label'].value_counts(normalize=True)*100, 0)\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "spam_pct.plot.barh(color='slateblue')\n",
    "ax.set_title('Spam vs. ham, %', fontsize=25)\n",
    "ax.set_xlabel(None)\n",
    "ax.tick_params(axis='both', labelsize=16, left=False)\n",
    "for side in ['top', 'right', 'left']:\n",
    "    ax.spines[side].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plenary glance at the data identifies that about 87% of the messages are ham, while the remaining 13% is spam.  At a high-level, this tracks with experience, since most messages that people receive are, in fact, ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Set\n",
    "\n",
    "Splitting the dataset into a training and a test set is next in the process, where the training set accounts for 80% of the data, and the test set for the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam vs. ham in the training set, %\n",
      "+-------+------+\n",
      "| Label |  %   |\n",
      "+-------+------+\n",
      "|  ham  | 87.0 |\n",
      "| spam  | 13.0 |\n",
      "+-------+------+\n",
      "\n",
      "\n",
      "Spam vs. ham in the test set, %\n",
      "+-------+------+\n",
      "| Label |  %   |\n",
      "+-------+------+\n",
      "|  ham  | 87.0 |\n",
      "| spam  | 13.0 |\n",
      "+-------+------+\n"
     ]
    }
   ],
   "source": [
    "sms_randomized = sms.sample(frac=1, random_state=1)\n",
    "\n",
    "# Creating a training set (80%) and a test set (20%)\n",
    "training_set = sms_randomized[:4458].reset_index(drop=True)\n",
    "test_set = sms_randomized[4458:].reset_index(drop=True)\n",
    "\n",
    "# Finding the % of spam and ham in both sets\n",
    "pretty_print_table(df=training_set, substring='in the training set')\n",
    "print('\\n')\n",
    "pretty_print_table(df=test_set, substring='in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets track with the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This next step requires the calculation of all probabilities the algorithm will need.  In order to do so, however, it is wise to clean the data appropriately.\n",
    "\n",
    "The main goal is to have a count of each unique word in the SMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Case and Punctuation\n",
    "\n",
    "First up is clearing both punctuation and ensuring all letters are lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before cleaning\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuation and making all the words lower case\n",
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W', ' ').str.lower()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary\n",
    "\n",
    "Next up is creating the lexicon, the list of unique words in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                  [yep, by, the, pretty, sculpture]\n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...\n",
       "2   ham                    [welp, apparently, he, retired]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary of the training set: 7,783\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sms in training_set['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = list(set(vocabulary))\n",
    "print(f'Number of unique words in the vocabulary of the training set: {len(vocabulary):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Training Set\n",
    "\n",
    "This final step includes using the vocabulary from above to make the final data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vilikkam</th>\n",
       "      <th>3750</th>\n",
       "      <th>reminding</th>\n",
       "      <th>clubmoby</th>\n",
       "      <th>becaus</th>\n",
       "      <th>wa14</th>\n",
       "      <th>anyways</th>\n",
       "      <th>checkin</th>\n",
       "      <th>score</th>\n",
       "      <th>october</th>\n",
       "      <th>...</th>\n",
       "      <th>forth</th>\n",
       "      <th>rushing</th>\n",
       "      <th>lazy</th>\n",
       "      <th>subscriptn3gbp</th>\n",
       "      <th>donno</th>\n",
       "      <th>max</th>\n",
       "      <th>okay</th>\n",
       "      <th>sir</th>\n",
       "      <th>psychic</th>\n",
       "      <th>mapquest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vilikkam  3750  reminding  clubmoby  becaus  wa14  anyways  checkin  score  \\\n",
       "0         0     0          0         0       0     0        0        0      0   \n",
       "1         0     0          0         0       0     0        0        0      0   \n",
       "2         0     0          0         0       0     0        0        0      0   \n",
       "\n",
       "   october  ...  forth  rushing  lazy  subscriptn3gbp  donno  max  okay  sir  \\\n",
       "0        0  ...      0        0     0               0      0    0     0    0   \n",
       "1        0  ...      0        0     0               0      0    0     0    0   \n",
       "2        0  ...      0        0     0               0      0    0     0    0   \n",
       "\n",
       "   psychic  mapquest  \n",
       "0        0         0  \n",
       "1        0         0  \n",
       "2        0         0  \n",
       "\n",
       "[3 rows x 7783 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dictionary where each key is a unique word from the vocabulary,\n",
    "# and each value is a list of the frequencies of that word in each message\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index]+=1\n",
    "        \n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>vilikkam</th>\n",
       "      <th>3750</th>\n",
       "      <th>reminding</th>\n",
       "      <th>clubmoby</th>\n",
       "      <th>becaus</th>\n",
       "      <th>wa14</th>\n",
       "      <th>anyways</th>\n",
       "      <th>checkin</th>\n",
       "      <th>...</th>\n",
       "      <th>forth</th>\n",
       "      <th>rushing</th>\n",
       "      <th>lazy</th>\n",
       "      <th>subscriptn3gbp</th>\n",
       "      <th>donno</th>\n",
       "      <th>max</th>\n",
       "      <th>okay</th>\n",
       "      <th>sir</th>\n",
       "      <th>psychic</th>\n",
       "      <th>mapquest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  vilikkam  3750  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]         0     0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...         0     0   \n",
       "2   ham                    [welp, apparently, he, retired]         0     0   \n",
       "\n",
       "   reminding  clubmoby  becaus  wa14  anyways  checkin  ...  forth  rushing  \\\n",
       "0          0         0       0     0        0        0  ...      0        0   \n",
       "1          0         0       0     0        0        0  ...      0        0   \n",
       "2          0         0       0     0        0        0  ...      0        0   \n",
       "\n",
       "   lazy  subscriptn3gbp  donno  max  okay  sir  psychic  mapquest  \n",
       "0     0               0      0    0     0    0        0         0  \n",
       "1     0               0      0    0     0    0        0         0  \n",
       "2     0               0      0    0     0    0        0         0  \n",
       "\n",
       "[3 rows x 7785 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_final = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Most Frequent Words in Spam Messages\n",
    "\n",
    "Having a count of the most frequently used words in the spam messages will provide some solid insight for testing the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 229. MiB for an array with shape (7783, 3858) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5cd9426a3d74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mspam_sms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_set_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_set_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mham_sms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_set_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_set_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'ham'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2891\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2893\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2895\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2945\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2946\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2947\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m         \"\"\"\n\u001b[1;32m-> 3363\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3350\u001b[1;33m         new_data = self._mgr.take(\n\u001b[0m\u001b[0;32m   3351\u001b[0m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3352\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m         return self.reindex_indexer(\n\u001b[0m\u001b[0;32m   1448\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate)\u001b[0m\n\u001b[0;32m   1282\u001b[0m             \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m             new_blocks = [\n\u001b[0m\u001b[0;32m   1285\u001b[0m                 blk.take_nd(\n\u001b[0;32m   1286\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1283\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m             new_blocks = [\n\u001b[1;32m-> 1285\u001b[1;33m                 blk.take_nd(\n\u001b[0m\u001b[0;32m   1286\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m   1256\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1706\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 229. MiB for an array with shape (7783, 3858) and data type int64"
     ]
    }
   ],
   "source": [
    "spam_sms = training_set_final[training_set_final['Label']=='spam']\n",
    "ham_sms = training_set_final[training_set_final['Label']=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of words from all spam messages with their frequencies\n",
    "spam_dict = {}\n",
    "for sms in spam_sms['SMS']:\n",
    "    for word in sms:\n",
    "        if word not in spam_dict:\n",
    "            spam_dict[word]=0\n",
    "        spam_dict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dictionary in descending order of word frequencies \n",
    "sorted_spam_dict = dict(sorted(spam_dict.items(), key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['call', 'free', 'stop', 'mobile', 'text', 'claim', 'www', \n",
    "            'prize', 'send', 'cash', 'nokia', 'win', 'urgent', 'service',\n",
    "            'contact', 'com', 'msg', 'chat', 'guaranteed', 'customer', \n",
    "            'awarded', 'sms', 'ringtone', 'video', 'rate', 'latest', \n",
    "            'award', 'code', 'camera', 'chance', 'apply', 'valid', 'selected',\n",
    "            'offer', 'tones', 'collection', 'mob', 'network', 'attempt', \n",
    "            'bonus', 'delivery', 'weekly', 'club', 'http', 'help', 'dating',\n",
    "            'vouchers', 'poly', 'auction', 'ltd', 'pounds', 'special',\n",
    "            'services', 'games', 'await', 'double', 'unsubscribe', 'hot',\n",
    "            'price', 'sexy', 'camcorder', 'content', 'top', 'calls', \n",
    "            'account', 'private', 'winner', 'savamob', 'offers', 'pobox',\n",
    "            'gift', 'net', 'quiz', 'expires', 'freemsg', 'play', 'ipod',\n",
    "            'last', 'order', 'anytime', 'congratulations', 'caller', 'points',\n",
    "            'identifier', 'voucher', 'statement', 'operator', 'real', \n",
    "            'mobiles', 'important', 'join', 'rental', 'valued', 'congrats',\n",
    "            'final', 'enjoy', 'unlimited', 'tv', 'charged', 'sex']\n",
    "\n",
    "# Extracting only the 100 most frequent spam words with their frequencies\n",
    "filtered_sorted_spam_dict = {}\n",
    "for word in selected:\n",
    "    if word in sorted_spam_dict:\n",
    "        filtered_sorted_spam_dict[word]=sorted_spam_dict[word] \n",
    "        \n",
    "print(f'The number of the most popular spam words selected: {len(filtered_sorted_spam_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word cloud\n",
    "fig = plt.subplots(figsize=(12,10)) \n",
    "wordcloud = WordCloud(width=1000, height=700,\n",
    "                      background_color='white', \n",
    "                      random_state=1).generate_from_frequencies(filtered_sorted_spam_dict)\n",
    "plt.title('The most frequent words in spam messages\\n', fontsize=29)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Constants\n",
    "\n",
    "Now, it's time to start creating the spam filter.\n",
    "\n",
    "When a new message comes in, the Naive Bayes algorithm will make the classification based on the probabilities it gets to these two equations:\n",
    "\n",
    "![alt text](formula1.png \"Classification Formulae A\")\n",
    "\n",
    "to calculate <i>P(wi|Spam)</i> and <i>P(wi|Ham)</i> inside the formulas above:\n",
    "\n",
    "![alt text](formula2.png \"Classification Formulae B\")\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "* <i>N<sub>wi|Spam</sub></i> — the number of times the word wi occurs in spam messages,\n",
    "* <i>N<sub>wi|Ham</sub></i> — the number of times the word wi occurs in ham messages,\n",
    "* <i>N<sub>Spam</sub></i> — total number of words in spam messages,\n",
    "* <i>N<sub>Ham</sub></i> — total number of words in ham messages,\n",
    "* <i>N<sub>Vocabulary</sub></i> — total number of unique words in the vocabulary,\n",
    "* <i>α</i> — a smoothing parameter.\n",
    "\n",
    "Of course some of these will have the same value for every new message: <i>P(Spam)</i>, <i>P(Ham)</i>, <i>N<sub>Spam</sub></i>, <i>N<sub>Ham</sub></i>, <i>N<sub>Vocabulary</sub></i>.  We can use Laplace smoothing and set our <i>α</i> value to 1.\n",
    "\n",
    "Now to calculate the constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = training_set_final['Label'].value_counts()['spam']/len(training_set_final)\n",
    "p_ham = training_set_final['Label'].value_counts()['ham']/len(training_set_final)\n",
    "\n",
    "n_spam = 0\n",
    "n_ham = 0\n",
    "for i in range(len(training_set_final)):\n",
    "    row = list(training_set_final.iloc[i].values)\n",
    "    for j in range(2,len(row)):\n",
    "        if row[0]=='spam':\n",
    "            n_spam+=row[j]\n",
    "        else:\n",
    "            n_ham+=row[j]\n",
    "            \n",
    "n_vocabulary = len(vocabulary)\n",
    "alpha = 1\n",
    "\n",
    "print(f'p_spam: {p_spam:.2f}\\n'\n",
    "      f'p_ham: {p_ham:.2f}\\n'\n",
    "      f'n_spam: {n_spam:,}\\n'\n",
    "      f'n_ham: {n_ham:,}\\n'\n",
    "      f'n_vocabulary: {n_vocabulary:,}\\n'\n",
    "      f'alpha: {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "The parameters <i>P(wi|Spam)</i> and <i>P(wi|Ham)</i> will vary depending on the individual words. However, both probabilities for each individual word remain constant for every new message, since they only depend on the training set. This means that we can use our training set to calculate both probabilities for each word in our vocabulary beforehand, which makes the Naive Bayes algorithm very fast compared to other algorithms. When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message.\n",
    "\n",
    "There are 7,783 words in our vocabulary, hence we'll need to calculate a total of 15,566 probabilities <i>(P(wi|Spam)</i> and <i>P(wi|Ham)</i> for each word) using the following equations:\n",
    "\n",
    "![alt text](formula3.png \"Parameter Calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wi_spam = {}\n",
    "p_wi_ham = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    p_wi_spam[word] = (spam_sms[word].sum()+alpha)/(n_spam+alpha*n_vocabulary)\n",
    "    p_wi_ham[word] = (ham_sms[word].sum()+alpha)/(n_ham+alpha*n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a New Message\n",
    "\n",
    "With the constants and parameters calculated, these can be converted into a spam filter.  The definition for this product is two-fold:\n",
    "\n",
    "* Ingests a new message as input\n",
    "* Calculates <i>P(Spam|message)</i> and <i>P(Ham|message)</i> using the following formulas:\n",
    "\n",
    "![alt text](formula4.png)\n",
    "\n",
    "* Compares both values and:\n",
    "    * if <i>P(Ham|message)</i> > <i>P(Spam|message)</i>, then the message is classified as ham,\n",
    "    * if <i>P(Ham|message)</i> < <i>P(Spam|message)</i>, then the message is classified as spam,\n",
    "    * if <i>P(Ham|message)</i> = <i>P(Spam|message)</i>, then the algorithm may request human help.\n",
    "\n",
    "If a new message contains some words that are not in the vocabulary, these words will be ignored for the purposes of calculating probabilities.\n",
    "\n",
    "And we can test the function with obviously spam or ham messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    '''Takes in a message as a string, removes punctuation, and makes all the\n",
    "    words lower case, calculates P(Spam|message) and P(Ham|message) based on\n",
    "    the constants and parameters calculated earlier in the project, compares\n",
    "    the two values and classifies the message as spam or ham, or requires \n",
    "    human classification. \n",
    "    '''\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    for word in message:\n",
    "        if word in p_wi_spam:\n",
    "            p_spam_given_message*=p_wi_spam[word]\n",
    "        if word in p_wi_ham:\n",
    "            p_ham_given_message*=p_wi_ham[word]\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "\n",
    "# Testing the function\n",
    "print(classify_test_set('Do you want to win an amazing super-prize today?'))\n",
    "print(classify_test_set('Ian, you look super-amazing today!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has distinguished the meaning successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "From the previous work, we have a test set of messages.  The algorithm will treat each message as new since it was not in the training data set.  The output will be a classification label which we can use to compare to the human-assigned label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['Predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the accuracy of predicted vs. actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of the spam filter\n",
    "correct = 0\n",
    "total = len(test_set)        # number of sms in the test set\n",
    "for row in test_set.iterrows():\n",
    "    if row[1]['Predicted']==row[1]['Label']:\n",
    "        correct+=1\n",
    "accuracy = correct/total*100\n",
    "print(f'The accuracy of the spam filter: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result, our assumption of 80% has been surpassed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrectly-Classified Messages\n",
    "\n",
    "We can see that there were some messages that were classified incorrectly.  Some manual review will help understand what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_spam = test_set[(test_set['Predicted']=='spam')&(test_set['Label']=='ham')].reset_index(drop=True)\n",
    "false_ham = test_set[(test_set['Predicted']=='ham')&(test_set['Label']=='spam')].reset_index(drop=True)\n",
    "unclear = test_set[test_set['Predicted']=='needs human classification'].reset_index(drop=True)\n",
    "\n",
    "print('Total number of wrongly classified messages: ', len(false_spam)+len(false_ham)+len(unclear))\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('FALSE SPAM MESSAGES:')\n",
    "for row in false_spam.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('FALSE HAM MESSAGES:')\n",
    "for row in false_ham.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('UNCLEAR MESSAGES:')\n",
    "for row in unclear.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In very rare occasions, ham messages can be incorrectly detected as spam when they are very short (considering also that some of the words from a new message can be absent in the vocabulary) and, at the same time, contain suspicious ad-style words, like unlimited, phone, calls, messages, contact, sent, that were previously found mostly in spam messages and that we observed, indeed, earlier on the word cloud. In addition, false spam messages, being very short, can contain seemingly neutral words (like July) but which were found in the training set only 1-2 times and, by coincidence, in spam messages.\n",
    "* Spam messages incorrectly detected as ham tend to be rather long and have a high percentage of \"normal\" words, which allows them to override the system. They usually contain contact details, websites, mentioning sums of money, words like asap, or they just can, in case of being short, consist of the words absent in the vocabulary.\n",
    "* The message that was not identified at all (and originally it was a ham message) is quite long and characterized by heavy usage of slang and abbreviations most probably absent in the vocabulary. As for the other words, the majority of them look neutral and could have been detected both in spam and ham messages. There are, though, some potentially suspicious words (saved, boost, secret, energy, instantly) that increased the probability of spam for this message up to being equal to that of ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Algorithm Case-Sensitive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_exp = sms_randomized[:4458].reset_index(drop=True)\n",
    "test_set_exp = sms_randomized[4458:].reset_index(drop=True)\n",
    "training_set_exp['SMS'] = training_set_exp['SMS'].str.replace('\\W', ' ')\n",
    "\n",
    "vocabulary_exp = []\n",
    "for sms in training_set_exp['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary_exp.append(word)\n",
    "vocabulary_exp = list(set(vocabulary_exp))\n",
    "\n",
    "word_counts_per_sms_exp = {unique_word: [0] * len(training_set_exp['SMS']) for unique_word in vocabulary_exp}\n",
    "for index, sms in enumerate(training_set_exp['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms_exp[word][index]+=1\n",
    "        \n",
    "word_counts_exp = pd.DataFrame(word_counts_per_sms_exp)\n",
    "\n",
    "training_set_final_exp = pd.concat([training_set_exp, word_counts_exp], axis=1)\n",
    "    \n",
    "spam_sms_exp = training_set_final_exp[training_set_final_exp['Label']=='spam']\n",
    "ham_sms_exp = training_set_final_exp[training_set_final_exp['Label']=='ham']\n",
    "\n",
    "p_spam_exp = training_set_final_exp['Label'].value_counts()['spam']/len(training_set_final_exp)\n",
    "p_ham_exp = training_set_final_exp['Label'].value_counts()['ham']/len(training_set_final_exp)\n",
    "\n",
    "n_spam_exp = 0\n",
    "n_ham_exp = 0\n",
    "for i in range(len(training_set_final_exp)):\n",
    "    row = list(training_set_final_exp.iloc[i].values)\n",
    "    for j in range(2,len(row)):\n",
    "        if row[0]=='spam':\n",
    "            n_spam_exp+=row[j]\n",
    "        else:\n",
    "            n_ham_exp+=row[j]\n",
    "            \n",
    "n_vocabulary_exp = len(vocabulary_exp)\n",
    "alpha = 1\n",
    "\n",
    "p_wi_spam_exp = {}\n",
    "p_wi_ham_exp = {}\n",
    "for word in vocabulary_exp:\n",
    "    p_wi_spam_exp[word] = (spam_sms_exp[word].sum()+alpha)/(n_spam_exp+alpha*n_vocabulary_exp)\n",
    "    p_wi_ham_exp[word] = (ham_sms_exp[word].sum()+alpha)/(n_ham_exp+alpha*n_vocabulary_exp)\n",
    "    \n",
    "def classify_test_set_exp(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.split()\n",
    "    p_spam_given_message_exp = p_spam_exp\n",
    "    p_ham_given_message_exp = p_ham_exp\n",
    "    for word in message:\n",
    "        if word in p_wi_spam_exp:\n",
    "            p_spam_given_message_exp*=p_wi_spam_exp[word]\n",
    "        if word in p_wi_ham_exp:\n",
    "            p_ham_given_message_exp*=p_wi_ham_exp[word]\n",
    "    if p_ham_given_message_exp > p_spam_given_message_exp:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message_exp > p_ham_given_message_exp:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "    \n",
    "test_set_exp['Predicted'] = test_set_exp['SMS'].apply(classify_test_set_exp)\n",
    "\n",
    "correct_exp = 0\n",
    "total_exp = len(test_set_exp)\n",
    "\n",
    "for row in test_set_exp.iterrows():\n",
    "    if row[1]['Predicted']==row[1]['Label']:\n",
    "        correct_exp+=1\n",
    "accuracy_exp = correct_exp/total_exp*100\n",
    "print(f'The accuracy of the spam filter: {accuracy_exp:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the experiment on making the filtering system more complex by introducing letter case sensitivity ended up rendering our spam filter much less efficient in labeling a new message (the accuracy has dropped by 13.5%), even though it's still more efficient than 80% of accuracy that we aimed at the beginning. It seems that the letter case doesn't really make any valuable difference when it comes to distinguishing between spam and ham messages. Hence, for further classifying new messages, we can approve the previous spam filter with 98.74% of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we created a highly accurate spam filter based on the multinomial Naive Bayes algorithm and a dataset of labeled 5,572 SMS. The spam filter takes in a new message and classifies it as spam or ham. We managed to reach an accuracy of 98.74%, which is almost 20% higher than our initial focus. Below are some additional conclusions and insights from this project:\n",
    "\n",
    "* A few messages classified incorrectly have some features in common. False spam messages tend to be very short, have the words absent in the vocabulary, contain typical spam-like words, or neutral words previously detected, by coincidence, only in spam messages. False ham messages tend to be rather long and have a high percentage of neutral words or the words absent in the vocabulary. In the undefined messages, we can expect an approximately proportional mixture of neutral and spam-like words.\n",
    "* The attempt to increase the accuracy even further by making the algorithm sensitive to letter case resulted, just the opposite, in rendering the spam filter much less efficient, with the accuracy dropped by 13.5%. It seems that the letter case doesn't make any valuable difference when it comes to distinguishing between spam and ham messages.\n",
    "* The 100 most popular meaningful spam-prone words revealed the following patterns: encouraging people to do further actions, promising them something alluring, urging them, having sexual context, inviting to visit some web resources, advertising various digital devices and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
