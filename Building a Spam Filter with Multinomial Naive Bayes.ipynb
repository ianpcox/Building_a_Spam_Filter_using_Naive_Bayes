{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Multinomial Naive Bayes\n",
    "\n",
    "The purpose of this project is to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The goal is to write a program that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "To train the algorithm, we'll use a dataset of 5,572 SMS messages that are already classified by humans. The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the papers authored by Tiago A. Almeida and José María Gómez Hidalgo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset\n",
    "\n",
    "The first step is to explore the data, starting with reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import operator\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "\n",
    "print(f'Number of SMS messages: {sms.shape[0]:,}')\n",
    "print(f'Number of missing values in the dataframe: {sms.isnull().sum().sum()}\\n')\n",
    "\n",
    "def pretty_print_table(df, substring):\n",
    "    '''Pretty-prints a table of the result of `value_counts` method (in % and\n",
    "    rounded) on the `Label` column of an input dataframe. Prints the title of\n",
    "    the table with an input substring incorporated.\n",
    "    '''\n",
    "    print(f'Spam vs. ham {substring}, %')\n",
    "    spam_ham_pct = round(df['Label'].value_counts(normalize=True)*100, 0)\n",
    "    print(spam_ham_pct.to_markdown(tablefmt='pretty', headers=['Label', '%']))\n",
    "\n",
    "# Pretty-printing % of spam and ham messages\n",
    "pretty_print_table(df=sms, substring='(non-spam)')\n",
    "\n",
    "# Plotting % of spam and ham messages\n",
    "spam_pct = round(sms['Label'].value_counts(normalize=True)*100, 0)\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "spam_pct.plot.barh(color='slateblue')\n",
    "ax.set_title('Spam vs. ham, %', fontsize=25)\n",
    "ax.set_xlabel(None)\n",
    "ax.tick_params(axis='both', labelsize=16, left=False)\n",
    "for side in ['top', 'right', 'left']:\n",
    "    ax.spines[side].set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plenary glance at the data identifies that about 87% of the messages are ham, while the remaining 13% is spam.  At a high-level, this tracks with experience, since most messages that people receive are, in fact, ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Set\n",
    "\n",
    "Splitting the dataset into a training and a test set is next in the process, where the training set accounts for 80% of the data, and the test set for the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_randomized = sms.sample(frac=1, random_state=1)\n",
    "\n",
    "# Creating a training set (80%) and a test set (20%)\n",
    "training_set = sms_randomized[:4458].reset_index(drop=True)\n",
    "test_set = sms_randomized[4458:].reset_index(drop=True)\n",
    "\n",
    "# Finding the % of spam and ham in both sets\n",
    "pretty_print_table(df=training_set, substring='in the training set')\n",
    "print('\\n')\n",
    "pretty_print_table(df=test_set, substring='in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets track with the expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This next step requires the calculation of all probabilities the algorithm will need.  In order to do so, however, it is wise to clean the data appropriately.\n",
    "\n",
    "The main goal is to have a count of each unique word in the SMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Case and Punctuation\n",
    "\n",
    "First up is clearing both punctuation and ensuring all letters are lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and making all the words lower case\n",
    "training_set['SMS'] = training_set['SMS'].str.replace('\\W', ' ').str.lower()\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary\n",
    "\n",
    "Next up is creating the lexicon, the list of unique words in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "training_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sms in training_set['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary.append(word)\n",
    "vocabulary = list(set(vocabulary))\n",
    "print(f'Number of unique words in the vocabulary of the training set: {len(vocabulary):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Training Set\n",
    "\n",
    "This final step includes using the vocabulary from above to make the final data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary where each key is a unique word from the vocabulary,\n",
    "# and each value is a list of the frequencies of that word in each message\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index]+=1\n",
    "        \n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_final = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Most Frequent Words in Spam Messages\n",
    "\n",
    "Having a count of the most frequently used words in the spam messages will provide some solid insight for testing the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sms = training_set_final[training_set_final['Label']=='spam']\n",
    "ham_sms = training_set_final[training_set_final['Label']=='ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of words from all spam messages with their frequencies\n",
    "spam_dict = {}\n",
    "for sms in spam_sms['SMS']:\n",
    "    for word in sms:\n",
    "        if word not in spam_dict:\n",
    "            spam_dict[word]=0\n",
    "        spam_dict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dictionary in descending order of word frequencies \n",
    "sorted_spam_dict = dict(sorted(spam_dict.items(), key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['call', 'free', 'stop', 'mobile', 'text', 'claim', 'www', \n",
    "            'prize', 'send', 'cash', 'nokia', 'win', 'urgent', 'service',\n",
    "            'contact', 'com', 'msg', 'chat', 'guaranteed', 'customer', \n",
    "            'awarded', 'sms', 'ringtone', 'video', 'rate', 'latest', \n",
    "            'award', 'code', 'camera', 'chance', 'apply', 'valid', 'selected',\n",
    "            'offer', 'tones', 'collection', 'mob', 'network', 'attempt', \n",
    "            'bonus', 'delivery', 'weekly', 'club', 'http', 'help', 'dating',\n",
    "            'vouchers', 'poly', 'auction', 'ltd', 'pounds', 'special',\n",
    "            'services', 'games', 'await', 'double', 'unsubscribe', 'hot',\n",
    "            'price', 'sexy', 'camcorder', 'content', 'top', 'calls', \n",
    "            'account', 'private', 'winner', 'savamob', 'offers', 'pobox',\n",
    "            'gift', 'net', 'quiz', 'expires', 'freemsg', 'play', 'ipod',\n",
    "            'last', 'order', 'anytime', 'congratulations', 'caller', 'points',\n",
    "            'identifier', 'voucher', 'statement', 'operator', 'real', \n",
    "            'mobiles', 'important', 'join', 'rental', 'valued', 'congrats',\n",
    "            'final', 'enjoy', 'unlimited', 'tv', 'charged', 'sex']\n",
    "\n",
    "# Extracting only the 100 most frequent spam words with their frequencies\n",
    "filtered_sorted_spam_dict = {}\n",
    "for word in selected:\n",
    "    if word in sorted_spam_dict:\n",
    "        filtered_sorted_spam_dict[word]=sorted_spam_dict[word] \n",
    "        \n",
    "print(f'The number of the most popular spam words selected: {len(filtered_sorted_spam_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word cloud\n",
    "fig = plt.subplots(figsize=(12,10)) \n",
    "wordcloud = WordCloud(width=1000, height=700,\n",
    "                      background_color='white', \n",
    "                      random_state=1).generate_from_frequencies(filtered_sorted_spam_dict)\n",
    "plt.title('The most frequent words in spam messages\\n', fontsize=29)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Constants\n",
    "\n",
    "Now, it's time to start creating the spam filter.\n",
    "\n",
    "When a new message comes in, the Naive Bayes algorithm will make the classification based on the probabilities it gets to these two equations:\n",
    "\n",
    "![alt text](formula1.png \"Classification Formulae A\")\n",
    "\n",
    "to calculate <i>P(wi|Spam)</i> and <i>P(wi|Ham)</i> inside the formulas above:\n",
    "\n",
    "![alt text](formula2.png \"Classification Formulae B\")\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "* <i>N<sub>wi|Spam</sub></i> — the number of times the word wi occurs in spam messages,\n",
    "* <i>N<sub>wi|Ham</sub></i> — the number of times the word wi occurs in ham messages,\n",
    "* <i>N<sub>Spam</sub></i> — total number of words in spam messages,\n",
    "* <i>N<sub>Ham</sub></i> — total number of words in ham messages,\n",
    "* <i>N<sub>Vocabulary</sub></i> — total number of unique words in the vocabulary,\n",
    "* <i>α</i> — a smoothing parameter.\n",
    "\n",
    "Of course some of these will have the same value for every new message: <i>P(Spam)</i>, <i>P(Ham)</i>, <i>N<sub>Spam</sub></i>, <i>N<sub>Ham</sub></i>, <i>N<sub>Vocabulary</sub></i>.  We can use Laplace smoothing and set our <i>α</i> value to 1.\n",
    "\n",
    "Now to calculate the constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = training_set_final['Label'].value_counts()['spam']/len(training_set_final)\n",
    "p_ham = training_set_final['Label'].value_counts()['ham']/len(training_set_final)\n",
    "\n",
    "n_spam = 0\n",
    "n_ham = 0\n",
    "for i in range(len(training_set_final)):\n",
    "    row = list(training_set_final.iloc[i].values)\n",
    "    for j in range(2,len(row)):\n",
    "        if row[0]=='spam':\n",
    "            n_spam+=row[j]\n",
    "        else:\n",
    "            n_ham+=row[j]\n",
    "            \n",
    "n_vocabulary = len(vocabulary)\n",
    "alpha = 1\n",
    "\n",
    "print(f'p_spam: {p_spam:.2f}\\n'\n",
    "      f'p_ham: {p_ham:.2f}\\n'\n",
    "      f'n_spam: {n_spam:,}\\n'\n",
    "      f'n_ham: {n_ham:,}\\n'\n",
    "      f'n_vocabulary: {n_vocabulary:,}\\n'\n",
    "      f'alpha: {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "The parameters <i>P(wi|Spam)</i> and <i>P(wi|Ham)</i> will vary depending on the individual words. However, both probabilities for each individual word remain constant for every new message, since they only depend on the training set. This means that we can use our training set to calculate both probabilities for each word in our vocabulary beforehand, which makes the Naive Bayes algorithm very fast compared to other algorithms. When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message.\n",
    "\n",
    "There are 7,783 words in our vocabulary, hence we'll need to calculate a total of 15,566 probabilities <i>(P(wi|Spam)</i> and <i>P(wi|Ham)</i> for each word) using the following equations:\n",
    "\n",
    "![alt text](formula3.png \"Parameter Calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wi_spam = {}\n",
    "p_wi_ham = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    p_wi_spam[word] = (spam_sms[word].sum()+alpha)/(n_spam+alpha*n_vocabulary)\n",
    "    p_wi_ham[word] = (ham_sms[word].sum()+alpha)/(n_ham+alpha*n_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a New Message\n",
    "\n",
    "With the constants and parameters calculated, these can be converted into a spam filter.  The definition for this product is two-fold:\n",
    "\n",
    "* Ingests a new message as input\n",
    "* Calculates <i>P(Spam|message)</i> and <i>P(Ham|message)</i> using the following formulas:\n",
    "\n",
    "![alt text](formula4.png)\n",
    "\n",
    "* Compares both values and:\n",
    "    * if <i>P(Ham|message)</i> > <i>P(Spam|message)</i>, then the message is classified as ham,\n",
    "    * if <i>P(Ham|message)</i> < <i>P(Spam|message)</i>, then the message is classified as spam,\n",
    "    * if <i>P(Ham|message)</i> = <i>P(Spam|message)</i>, then the algorithm may request human help.\n",
    "\n",
    "If a new message contains some words that are not in the vocabulary, these words will be ignored for the purposes of calculating probabilities.\n",
    "\n",
    "And we can test the function with obviously spam or ham messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    '''Takes in a message as a string, removes punctuation, and makes all the\n",
    "    words lower case, calculates P(Spam|message) and P(Ham|message) based on\n",
    "    the constants and parameters calculated earlier in the project, compares\n",
    "    the two values and classifies the message as spam or ham, or requires \n",
    "    human classification. \n",
    "    '''\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    for word in message:\n",
    "        if word in p_wi_spam:\n",
    "            p_spam_given_message*=p_wi_spam[word]\n",
    "        if word in p_wi_ham:\n",
    "            p_ham_given_message*=p_wi_ham[word]\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "\n",
    "# Testing the function\n",
    "print(classify_test_set('Do you want to win an amazing super-prize today?'))\n",
    "print(classify_test_set('Ian, you look super-amazing today!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has distinguished the meaning successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "From the previous work, we have a test set of messages.  The algorithm will treat each message as new since it was not in the training data set.  The output will be a classification label which we can use to compare to the human-assigned label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['Predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the accuracy of predicted vs. actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of the spam filter\n",
    "correct = 0\n",
    "total = len(test_set)        # number of sms in the test set\n",
    "for row in test_set.iterrows():\n",
    "    if row[1]['Predicted']==row[1]['Label']:\n",
    "        correct+=1\n",
    "accuracy = correct/total*100\n",
    "print(f'The accuracy of the spam filter: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result, our assumption of 80% has been surpassed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrectly-Classified Messages\n",
    "\n",
    "We can see that there were some messages that were classified incorrectly.  Some manual review will help understand what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_spam = test_set[(test_set['Predicted']=='spam')&(test_set['Label']=='ham')].reset_index(drop=True)\n",
    "false_ham = test_set[(test_set['Predicted']=='ham')&(test_set['Label']=='spam')].reset_index(drop=True)\n",
    "unclear = test_set[test_set['Predicted']=='needs human classification'].reset_index(drop=True)\n",
    "\n",
    "print('Total number of wrongly classified messages: ', len(false_spam)+len(false_ham)+len(unclear))\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('FALSE SPAM MESSAGES:')\n",
    "for row in false_spam.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('FALSE HAM MESSAGES:')\n",
    "for row in false_ham.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('UNCLEAR MESSAGES:')\n",
    "for row in unclear.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In very rare occasions, ham messages can be incorrectly detected as spam when they are very short (considering also that some of the words from a new message can be absent in the vocabulary) and, at the same time, contain suspicious ad-style words, like unlimited, phone, calls, messages, contact, sent, that were previously found mostly in spam messages and that we observed, indeed, earlier on the word cloud. In addition, false spam messages, being very short, can contain seemingly neutral words (like July) but which were found in the training set only 1-2 times and, by coincidence, in spam messages.\n",
    "* Spam messages incorrectly detected as ham tend to be rather long and have a high percentage of \"normal\" words, which allows them to override the system. They usually contain contact details, websites, mentioning sums of money, words like asap, or they just can, in case of being short, consist of the words absent in the vocabulary.\n",
    "* The message that was not identified at all (and originally it was a ham message) is quite long and characterized by heavy usage of slang and abbreviations most probably absent in the vocabulary. As for the other words, the majority of them look neutral and could have been detected both in spam and ham messages. There are, though, some potentially suspicious words (saved, boost, secret, energy, instantly) that increased the probability of spam for this message up to being equal to that of ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Algorithm Case-Sensitive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_exp = sms_randomized[:4458].reset_index(drop=True)\n",
    "test_set_exp = sms_randomized[4458:].reset_index(drop=True)\n",
    "training_set_exp['SMS'] = training_set_exp['SMS'].str.replace('\\W', ' ')\n",
    "\n",
    "vocabulary_exp = []\n",
    "for sms in training_set_exp['SMS']:\n",
    "    for word in sms:\n",
    "        vocabulary_exp.append(word)\n",
    "vocabulary_exp = list(set(vocabulary_exp))\n",
    "\n",
    "word_counts_per_sms_exp = {unique_word: [0] * len(training_set_exp['SMS']) for unique_word in vocabulary_exp}\n",
    "for index, sms in enumerate(training_set_exp['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms_exp[word][index]+=1\n",
    "        \n",
    "word_counts_exp = pd.DataFrame(word_counts_per_sms_exp)\n",
    "\n",
    "training_set_final_exp = pd.concat([training_set_exp, word_counts_exp], axis=1)\n",
    "    \n",
    "spam_sms_exp = training_set_final_exp[training_set_final_exp['Label']=='spam']\n",
    "ham_sms_exp = training_set_final_exp[training_set_final_exp['Label']=='ham']\n",
    "\n",
    "p_spam_exp = training_set_final_exp['Label'].value_counts()['spam']/len(training_set_final_exp)\n",
    "p_ham_exp = training_set_final_exp['Label'].value_counts()['ham']/len(training_set_final_exp)\n",
    "\n",
    "n_spam_exp = 0\n",
    "n_ham_exp = 0\n",
    "for i in range(len(training_set_final_exp)):\n",
    "    row = list(training_set_final_exp.iloc[i].values)\n",
    "    for j in range(2,len(row)):\n",
    "        if row[0]=='spam':\n",
    "            n_spam_exp+=row[j]\n",
    "        else:\n",
    "            n_ham_exp+=row[j]\n",
    "            \n",
    "n_vocabulary_exp = len(vocabulary_exp)\n",
    "alpha = 1\n",
    "\n",
    "p_wi_spam_exp = {}\n",
    "p_wi_ham_exp = {}\n",
    "for word in vocabulary_exp:\n",
    "    p_wi_spam_exp[word] = (spam_sms_exp[word].sum()+alpha)/(n_spam_exp+alpha*n_vocabulary_exp)\n",
    "    p_wi_ham_exp[word] = (ham_sms_exp[word].sum()+alpha)/(n_ham_exp+alpha*n_vocabulary_exp)\n",
    "    \n",
    "def classify_test_set_exp(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.split()\n",
    "    p_spam_given_message_exp = p_spam_exp\n",
    "    p_ham_given_message_exp = p_ham_exp\n",
    "    for word in message:\n",
    "        if word in p_wi_spam_exp:\n",
    "            p_spam_given_message_exp*=p_wi_spam_exp[word]\n",
    "        if word in p_wi_ham_exp:\n",
    "            p_ham_given_message_exp*=p_wi_ham_exp[word]\n",
    "    if p_ham_given_message_exp > p_spam_given_message_exp:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message_exp > p_ham_given_message_exp:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "    \n",
    "test_set_exp['Predicted'] = test_set_exp['SMS'].apply(classify_test_set_exp)\n",
    "\n",
    "correct_exp = 0\n",
    "total_exp = len(test_set_exp)\n",
    "\n",
    "for row in test_set_exp.iterrows():\n",
    "    if row[1]['Predicted']==row[1]['Label']:\n",
    "        correct_exp+=1\n",
    "accuracy_exp = correct_exp/total_exp*100\n",
    "print(f'The accuracy of the spam filter: {accuracy_exp:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the experiment on making the filtering system more complex by introducing letter case sensitivity ended up rendering our spam filter much less efficient in labeling a new message (the accuracy has dropped by 13.5%), even though it's still more efficient than 80% of accuracy that we aimed at the beginning. It seems that the letter case doesn't really make any valuable difference when it comes to distinguishing between spam and ham messages. Hence, for further classifying new messages, we can approve the previous spam filter with 98.74% of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we created a highly accurate spam filter based on the multinomial Naive Bayes algorithm and a dataset of labeled 5,572 SMS. The spam filter takes in a new message and classifies it as spam or ham. We managed to reach an accuracy of 98.74%, which is almost 20% higher than our initial focus. Below are some additional conclusions and insights from this project:\n",
    "\n",
    "* A few messages classified incorrectly have some features in common. False spam messages tend to be very short, have the words absent in the vocabulary, contain typical spam-like words, or neutral words previously detected, by coincidence, only in spam messages. False ham messages tend to be rather long and have a high percentage of neutral words or the words absent in the vocabulary. In the undefined messages, we can expect an approximately proportional mixture of neutral and spam-like words.\n",
    "* The attempt to increase the accuracy even further by making the algorithm sensitive to letter case resulted, just the opposite, in rendering the spam filter much less efficient, with the accuracy dropped by 13.5%. It seems that the letter case doesn't make any valuable difference when it comes to distinguishing between spam and ham messages.\n",
    "* The 100 most popular meaningful spam-prone words revealed the following patterns: encouraging people to do further actions, promising them something alluring, urging them, having sexual context, inviting to visit some web resources, advertising various digital devices and products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath, encoding='latin-1')\n",
    "\n",
    "data = load_data(\"spam.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_message(message):\n",
    "    return re.sub('\\W', ' ', message.lower())\n",
    "\n",
    "data['processed'] = data['message_column_name'].apply(preprocess_message)  ## Replace 'message_column_name' with the appropriate column name in your dataset that contains the SMS messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['processed'], \n",
    "    data['label_column_name'], ## Replace 'label_column_name' with the column name in your dataset that contains the labels (i.e., 'spam' or 'ham').\n",
    "    test_size=0.2, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def print_confusion_matrix(conf_matrix):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(\"                 Predicted:\")\n",
    "    print(\"                 Ham | Spam\")\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Actual Ham  |  {conf_matrix[0][0]}   | {conf_matrix[0][1]}\")\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Actual Spam |  {conf_matrix[1][0]}   | {conf_matrix[1][1]}\")\n",
    "\n",
    "print_confusion_matrix(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "# The confusion matrix is arranged as:\n",
    "\n",
    "# [TN, FP]\n",
    "# [FN, TP]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(messages)  # Assuming 'messages' is your list of SMSes\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(messages)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla RNN model\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 64, input_length=100),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 64, input_length=100),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model for text\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 64, input_length=100),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.fit(padded_sequences, labels, epochs=10, validation_split=0.1)  # Assuming 'labels' is your list of binary labels (1 for spam, 0 for ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the SVM classifier. Using the 'linear' kernel is common for text classification.\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test set\n",
    "y_pred_svm = svm_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'SVM Model Accuracy: {accuracy_svm * 100:.2f}%')\n",
    "\n",
    "# Displaying the confusion matrix for the SVM model\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print_confusion_matrix(conf_matrix_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Data Preparation\n",
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "data = data[['v1', 'v2']]\n",
    "data.columns = ['label', 'message']\n",
    "\n",
    "# Train-Validation-Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data['message'], data['label'], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 1. Deep Learning Models:\n",
    "# TODO: Implement the RNN, LSTM, and CNN models here. \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Conv1D, MaxPooling1D, Dense, Flatten\n",
    "\n",
    "# Tokenization and Sequence Padding\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train_padded = pad_sequences(sequences_train, maxlen=max_len, padding='post', truncating='post')\n",
    "X_val_padded = pad_sequences(sequences_val, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Vanilla RNN\n",
    "model_rnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(X_train_padded, y_train, epochs=5, validation_data=(X_val_padded, y_val))\n",
    "\n",
    "# LSTM\n",
    "model_lstm = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_padded, y_train, epochs=5, validation_data=(X_val_padded, y_val))\n",
    "\n",
    "# CNN\n",
    "model_cnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Conv1D(32, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train_padded, y_train, epochs=5, validation_data=(X_val_padded, y_val))\n",
    "\n",
    "\n",
    "# 2. Word Embeddings - BERT:\n",
    "# Define dataset for BERT\n",
    "class SMSTextDataset(Dataset):\n",
    "    def __init__(self, messages, labels, tokenizer, max_len):\n",
    "        self.messages = messages\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.messages)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        message = str(self.messages[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            message,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'message_text': message,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Training BERT for SMS Classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Simplifying training loop for BERT (detailed code with Trainer would be added)\n",
    "# TODO: Train the BERT model using Trainer or custom training loop.\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Data Preprocessing for BERT\n",
    "label_map = {'ham': 0, 'spam': 1}\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding='max_length', max_length=100)\n",
    "val_encodings = tokenizer(list(X_val), truncation=True, padding='max_length', max_length=100)\n",
    "train_labels = y_train.map(label_map).tolist()\n",
    "val_labels = y_val.map(label_map).tolist()\n",
    "\n",
    "class SMSTextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = SMSTextDataset(train_encodings, train_labels)\n",
    "val_dataset = SMSTextDataset(val_encodings, val_labels)\n",
    "\n",
    "# BERT Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# 3. Ensemble Methods:\n",
    "# Convert messages into TF-IDF representation\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# XGBoost\n",
    "xgb = GradientBoostingClassifier()\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# AdaBoost\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 4. SVM with TF-IDF:\n",
    "svm = SVC(probability=True)  # Set probability=True for AUC-ROC later on\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 5. Regularization Techniques:\n",
    "# TODO: While implementing deep learning models, add dropout and other regularization methods.\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_rnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    SimpleRNN(32, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    LSTM(32, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Conv1D(32, 5, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))\n",
    "])\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Get BERT's output\n",
    "output = bert_model.layers[0].output\n",
    "\n",
    "# Add dropout for regularization\n",
    "output = Dropout(0.2)(output[1])  # [1] captures the pooled_output from BERT\n",
    "\n",
    "# Add a dense classification layer with L2 regularization\n",
    "classifier = Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(output)\n",
    "\n",
    "# Construct the new model\n",
    "model = Model(inputs=bert_model.input, outputs=classifier)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# ... continue with fine-tuning\n",
    "\n",
    "\n",
    "# 6. Transfer Learning:\n",
    "# Implemented above with BERT\n",
    "\n",
    "# 7. Data Augmentation:\n",
    "# TODO: Implement data augmentation techniques. \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def synonym_replacement(text):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words]))\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "    if num_replaced > 0:\n",
    "        new_text = ' '.join(new_words)\n",
    "        return new_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Example:\n",
    "# augmented_text = synonym_replacement(\"This is a sample text\")\n",
    "\n",
    "# Using googletrans library for this example, but there are many others\n",
    "from googletrans import Translator\n",
    "\n",
    "def back_translate(text, lang='es'):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(text, dest=lang).text\n",
    "    translated_back_text = translator.translate(translated_text, dest='en').text\n",
    "    return translated_back_text\n",
    "\n",
    "# Example:\n",
    "# augmented_text = back_translate(\"This is a sample text\")\n",
    "\n",
    "import random\n",
    "\n",
    "def random_deletion(text, p=0.2):  # p is the probability of deleting a word\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p, words))\n",
    "    if len(remaining) == 0:  # If all words are deleted, choose one at random\n",
    "        return [random.choice(words)]\n",
    "    else:\n",
    "        return ' '.join(remaining)\n",
    "\n",
    "# Example:\n",
    "# augmented_text = random_deletion(\"This is a sample text\")\n",
    "\n",
    "def random_swap(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 2:\n",
    "        return text\n",
    "    idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "    words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example:\n",
    "# augmented_text = random_swap(\"This is a sample text\")\n",
    "\n",
    "\n",
    "# 8. Model Stacking:\n",
    "# Getting predictions on validation set\n",
    "val_preds_rf = rf.predict(X_val_tfidf)\n",
    "val_preds_xgb = xgb.predict(X_val_tfidf)\n",
    "val_preds_ada = ada.predict(X_val_tfidf)\n",
    "# TODO: Get BERT and SVM predictions on validation set\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming you've already loaded the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('path_to_saved_model')  # Replace 'path_to_saved_model' with the path to your fine-tuned BERT model\n",
    "\n",
    "def get_bert_predictions(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    logits = outputs[0]\n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)  # Convert softmax outputs to class labels\n",
    "    return predictions\n",
    "\n",
    "bert_predictions = get_bert_predictions(validation_texts)  # Replace 'validation_texts' with your actual validation data\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Assuming you've already instantiated and fit a TfidfVectorizer and trained an SVM\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Example initialization\n",
    "vectorizer.fit(training_texts)  # Replace 'training_texts' with your training data texts\n",
    "\n",
    "svm_model = SVC(probability=True)\n",
    "svm_model.fit(X_train_tfidf, y_train)  # Assuming X_train_tfidf is your training data transformed by the vectorizer\n",
    "\n",
    "def get_svm_predictions(texts):\n",
    "    tfidf_texts = vectorizer.transform(texts)\n",
    "    predictions = svm_model.predict(tfidf_texts)\n",
    "    return predictions\n",
    "\n",
    "svm_predictions = get_svm_predictions(validation_texts)\n",
    "\n",
    "\n",
    "# Stacking models\n",
    "X_train_stack = np.column_stack((val_preds_rf, val_preds_xgb, val_preds_ada))  # Add other models' predictions as needed\n",
    "model_stack = LogisticRegression().fit(X_train_stack, y_val)\n",
    "\n",
    "# Predictions on test set\n",
    "test_preds_rf = rf.predict(X_test_tfidf)\n",
    "test_preds_xgb = xgb.predict(X_test_tfidf)\n",
    "test_preds_ada = ada.predict(X_test_tfidf)\n",
    "# TODO: Get BERT and SVM predictions on test set\n",
    "\n",
    "# You've already loaded the tokenizer and model during the validation step\n",
    "\n",
    "def get_bert_test_predictions(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"tf\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    logits = outputs[0]\n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)  # Convert softmax outputs to class labels\n",
    "    return predictions\n",
    "\n",
    "bert_test_predictions = get_bert_test_predictions(test_texts)  # Replace 'test_texts' with your actual test data\n",
    "\n",
    "\n",
    "def get_svm_test_predictions(texts):\n",
    "    tfidf_texts = vectorizer.transform(texts)\n",
    "    predictions = svm_model.predict(tfidf_texts)\n",
    "    return predictions\n",
    "\n",
    "svm_test_predictions = get_svm_test_predictions(test_texts)\n",
    "\n",
    "\n",
    "X_test_stack = np.column_stack((test_preds_rf, test_preds_xgb, test_preds_ada))\n",
    "y_pred_stack = model_stack.predict(X_test_stack)\n",
    "\n",
    "# Evaluation\n",
    "# TODO: Evaluate all models using the mentioned metrics (AUC-ROC, precision, recall, F1 score, and accuracy).\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    # AUC-ROC\n",
    "    # If your labels are binary (e.g., 0 or 1), then you can directly compute AUC-ROC\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Precision\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    # Recall\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"AUC-ROC\": auc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "# Assuming `y_test` contains your true labels and `bert_test_predictions` & `svm_test_predictions` are your model predictions\n",
    "bert_metrics = evaluate_model(y_test, bert_test_predictions)\n",
    "svm_metrics = evaluate_model(y_test, svm_test_predictions)\n",
    "\n",
    "# Print out the metrics\n",
    "print(\"BERT Model Evaluation:\")\n",
    "for metric, value in bert_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nSVM Model Evaluation:\")\n",
    "for metric, value in svm_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    padding=True, max_length=128)\n",
    "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    logits = outputs[0]\n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    return predictions\n",
    "\n",
    "bert_test_predictions = get_bert_test_predictions(X_test)\n",
    "\n",
    "# Since you've already instantiated and fit a TfidfVectorizer and trained an SVM before, no need to reinitialize them\n",
    "def get_svm_test_predictions(texts):\n",
    "    tfidf_texts = vectorizer.transform(texts)\n",
    "    predictions = svm_model.predict(tfidf_texts)\n",
    "    return predictions\n",
    "\n",
    "svm_test_predictions = get_svm_test_predictions(X_test)\n",
    "\n",
    "# Stacking models for predictions on test set\n",
    "X_test_stack = np.column_stack((test_preds_rf, test_preds_xgb, test_preds_ada, bert_test_predictions, svm_test_predictions))\n",
    "stacked_test_predictions = model_stack.predict(X_test_stack)\n",
    "\n",
    "# To evaluate\n",
    "accuracy = accuracy_score(y_test, stacked_test_predictions)\n",
    "roc_auc = roc_auc_score(y_test, stacked_test_predictions)\n",
    "precision = precision_score(y_test, stacked_test_predictions)\n",
    "recall = recall_score(y_test, stacked_test_predictions)\n",
    "f1 = f1_score(y_test, stacked_test_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
